dirs:
    train:
        data: /data/sxu/easton/data/LibriSpeech/train-clean-100.phone.random.csv
        # data: /data/sxu/easton/data/LibriSpeech/tfdata/train_39/1.csv
        tfdata: /data/sxu/easton/data/LibriSpeech/tfdata/train
    dev:
        data: /data/sxu/easton/data/LibriSpeech/dev1000.phone.csv
        tfdata: /data/sxu/easton/data/LibriSpeech/tfdata/dev
    test:
        data: /data/sxu/easton/data/LibriSpeech/dev1000.phone.csv
    type: csv
    vocab: /data/sxu/easton/data/LibriSpeech/phones_42.txt
    ngram: /data/sxu/easton/data/LibriSpeech/100h.5gram
    restore: /data/sxu/easton/projects/EODM/models/libri_lm/checkpoint/


data:
    dim_embedding: 64
    unit: word

model:
    structure: lstm
    training_type: teacher-forcing
    loss_type: CE
    num_hidden: 256
    num_layers: 3

opti:
    type: adam
    warmup_steps: 100
    peak: 0.001
    decay_steps: 1000

dev_step: 100
decode_step: 50
save_step: 50

gpus: '0'
# gpus: '0,1,2,3'
num_batch_tokens: 120000
batch_size: 1000
# batch_size: 1000
bucket_boundaries: 100,150,200,250,300

num_epochs: 100000
num_steps: 500000

grad_clip_global_norm: 0.0
